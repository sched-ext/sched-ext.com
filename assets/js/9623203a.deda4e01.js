"use strict";(self.webpackChunksched_ext=self.webpackChunksched_ext||[]).push([[341],{7807:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"OVERVIEW","title":"Overview","description":"sched_ext is a Linux kernel feature which","source":"@site/docs/OVERVIEW.md","sourceDirName":".","slug":"/OVERVIEW","permalink":"/docs/OVERVIEW","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/OVERVIEW.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","next":{"title":"SCHED_EXT SCHEDULERS","permalink":"/docs/scheds/"}}');var t=s(4848),a=s(8453);const o={},r="Overview",l={},c=[{value:"1. Ease of experimentation and exploration",id:"1-ease-of-experimentation-and-exploration",level:2},{value:"Why is exploration important?",id:"why-is-exploration-important",level:3},{value:"Why can&#39;t we just explore directly with CFS?",id:"why-cant-we-just-explore-directly-with-cfs",level:3},{value:"How does sched_ext help with exploration?",id:"how-does-sched_ext-help-with-exploration",level:3},{value:"2. Customization",id:"2-customization",level:2},{value:"Why deploy custom schedulers rather than improving CFS?",id:"why-deploy-custom-schedulers-rather-than-improving-cfs",level:3},{value:"Won&#39;t this increase fragmentation?",id:"wont-this-increase-fragmentation",level:3},{value:"3. Rapid scheduler deployments",id:"3-rapid-scheduler-deployments",level:2},{value:"Dispatch queues",id:"dispatch-queues",level:3},{value:"Scheduling cycle",id:"scheduling-cycle",level:3},{value:"Verifying callback behavior",id:"verifying-callback-behavior",level:3}];function h(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"overview",children:"Overview"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://github.com/sched-ext/scx",children:"sched_ext"})," is a Linux kernel feature which\nenables implementing and dynamically loading safe kernel thread schedulers in\nBPF."]}),"\n",(0,t.jsx)(n.p,{children:"The benefits of such a framework are multifaceted, with there being three main\naxes where sched_ext is specifically designed to provide significant value:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Ease of experimentation and exploration: Enabling rapid iteration of new\nscheduling policies."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Customization: Building application-specific schedulers which implement\npolicies that are not applicable to general-purpose schedulers."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Rapid scheduler deployments: Non-disruptive swap outs of scheduling\npolicies in production environments."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["We'll begin by doing a deeper dive into the motivation of sched_ext in the\nfollowing ",(0,t.jsx)(n.a,{href:"#motivation",children:"Motivation"})," section. Following that, we'll provide some\ndeatils on the overall architecture of sched_ext in the ",(0,t.jsx)(n.a,{href:"#how",children:"How"})," section\nbelow."]}),"\n",(0,t.jsxs)(n.h1,{id:"motivation",children:["Motivation",(0,t.jsx)(n.a,{name:"motivation"})]}),"\n",(0,t.jsx)(n.h2,{id:"1-ease-of-experimentation-and-exploration",children:"1. Ease of experimentation and exploration"}),"\n",(0,t.jsx)(n.h3,{id:"why-is-exploration-important",children:"Why is exploration important?"}),"\n",(0,t.jsx)(n.p,{children:"Scheduling is a challenging problem space. Small changes in scheduling behavior\ncan have a significant impact on various components of a system, with the\ncorresponding effects varying widely across different platforms, architectures,\nand workloads."}),"\n",(0,t.jsx)(n.p,{children:"While complexities have always existed in scheduling, they have increased\ndramatically over the past 10-15 years. In the mid-late 2000s, cores were\ntypically homogeneous and further apart from each other, with the criteria for\nscheduling being roughly the same across the entire die."}),"\n",(0,t.jsx)(n.p,{children:"Systems in the modern age are by comparison much more complex. Modern CPU\ndesigns, where the total power budget of all CPU cores often far exceeds the\npower budget of the socket, with dynamic frequency scaling, and with or without\nchiplets, have significantly expanded the scheduling problem space.  Cache\nhierarchies have become less uniform, with Core Complex (CCX) designs such as\nrecent AMD processors having multiple shared L3 caches within a single socket.\nSuch topologies resemble NUMA sans persistent NUMA node stickiness."}),"\n",(0,t.jsx)(n.p,{children:"Use-cases have become increasingly complex and diverse as well. Applications\nsuch as mobile and VR have strict latency requirements to avoid missing\ndeadlines that impact user experience. Stacking workloads in servers is\nconstantly pushing the demands on the scheduler in terms of workload isolation\nand resource distribution."}),"\n",(0,t.jsxs)(n.p,{children:["Experimentation and exploration are important for any non-trivial problem\nspace. However, given the recent hardware and software developments, we believe\nthat experimentation and exploration are not just important, but ",(0,t.jsx)(n.em,{children:"critical"})," in\nthe scheduling problem space."]}),"\n",(0,t.jsxs)(n.p,{children:["Indeed, other approaches in industry are already being explored. AMD has\nproposed an experimental ",(0,t.jsx)(n.a,{href:"https://lore.kernel.org/lkml/20220910105326.1797-1-kprateek.nayak@amd.com/",children:"patch\nset"}),'\nwhich enables userspace to provide hints to the scheduler via "Userspace\nHinting". The approach adds a prctl() API which allows callers to set a\nnumerical "hint" value on a struct task_struct. This hint is then optionally\nread by the scheduler to adjust the cost calculus for various scheduling\ndecisions.']}),"\n",(0,t.jsxs)(n.p,{children:["Huawei have also ",(0,t.jsx)(n.a,{href:"https://lore.kernel.org/bpf/dedc7b72-9da4-91d0-d81d-75360c177188@huawei.com/",children:"expressed\ninterest"}),"\nin enabling some form of programmable scheduling. While we're unaware of any\npatch sets which have been sent to the upstream list for this proposal, it\nsimilarly illustrates the need for more flexibility in the scheduler."]}),"\n",(0,t.jsxs)(n.p,{children:["Additionally, Google has developed\n",(0,t.jsx)(n.a,{href:"https://dl.acm.org/doi/pdf/10.1145/3477132.3483542",children:"ghOSt"})," with the goal of\nenabling custom, userspace driven scheduling policies. Prior\n",(0,t.jsx)(n.a,{href:"https://lpc.events/event/16/contributions/1365/",children:"presentations"})," at LPC have\ndiscussed ghOSt and how BPF can be used to accelerate scheduling."]}),"\n",(0,t.jsx)(n.h3,{id:"why-cant-we-just-explore-directly-with-cfs",children:"Why can't we just explore directly with CFS?"}),"\n",(0,t.jsx)(n.p,{children:"Experimenting with CFS directly or implementing a new sched_class from scratch\nis of course possible, but is often difficult and time consuming. Newcomers to\nthe scheduler often require years to understand the codebase and become\nproductive contributors. Even for seasoned kernel engineers, experimenting with\nand upstreaming features can take a very long time. The iteration process\nitself is also time consuming, as testing scheduler changes on real hardware\nrequires reinstalling the kernel and rebooting the host."}),"\n",(0,t.jsx)(n.p,{children:"Core scheduling is an example of a feature that took a significant amount of\ntime and effort to integrate into the kernel. Part of the difficulty with core\nscheduling was the inherent mismatch in abstraction between the desire to\nperform core-wide scheduling, and the per-cpu design of the kernel scheduler.\nThis caused issues, for example ensuring proper fairness between the\nindependent runqueues of SMT siblings."}),"\n",(0,t.jsxs)(n.p,{children:["The high barrier to entry for working on the scheduler is an impediment to\nacademia as well. Master's/PhD candidates who are interested in improving the\nscheduler will spend years ramping-up, only to complete their degrees just as\nthey're finally ready to make significant changes. A lower entrance barrier\nwould allow researchers to more quickly ramp up, test out hypotheses, and\niterate on novel ideas. Research methodology is also severely hampered by the\nhigh barrier of entry to make modifications; for example, the\n",(0,t.jsx)(n.a,{href:"https://www.usenix.org/system/files/nsdi19-ousterhout.pdf",children:"Shenango"})," and\nShinjuku scheduling policies used sched affinity to replicate the desired\npolicy semantics, due to the difficulty of incorporating these policies into\nthe kernel directly."]}),"\n",(0,t.jsx)(n.p,{children:"The iterative process itself also imposes a significant cost to working on the\nscheduler. Testing changes requires developers to recompile and reinstall the\nkernel, reboot their machines, rewarm their workloads, and then finally rerun\ntheir benchmarks. Though some of this overhead could potentially be mitigated\nby enabling schedulers to be implemented as kernel modules, a machine crash or\nsubtle system state corruption is always only one innocuous mistake away.\nThese problems are exacerbated when testing production workloads in a\ndatacenter environment as well, where multiple hosts may be involved in an\nexperiment; requiring a significantly longer ramp up time. Warming up memcache\ninstances in the Meta production environment takes hours, for example."}),"\n",(0,t.jsx)(n.h3,{id:"how-does-sched_ext-help-with-exploration",children:"How does sched_ext help with exploration?"}),"\n",(0,t.jsx)(n.p,{children:"sched_ext attempts to address all of the problems described above. In this\nsection, we'll describe the benefits to experimentation and exploration that\nare afforded by sched_ext, provide real-world examples of those benefits, and\ndiscuss some of the trade-offs and considerations in our design choices."}),"\n",(0,t.jsx)(n.p,{children:"One of our main goals was to lower the barrier to entry for experimenting\nwith the scheduler. sched_ext provides ergonomic callbacks and helpers to\nease common operations such as managing idle CPUs, scheduling tasks on\narbitrary CPUs, handling preemptions from other scheduling classes, and\nmore. While sched_ext does require some ramp-up, the complexity is\nself-contained, and the learning curve gradual. Developers can ramp up by\nfirst implementing simple policies such as global weighted vtime scheduling\nin only tens of lines of code, and then continue to learn the APIs and\nbuilding blocks available with sched_ext as they build more featureful and\ncomplex schedulers."}),"\n",(0,t.jsx)(n.p,{children:"Another critical advantage provided by sched_ext is the use of BPF. BPF\nprovides strong safety guarantees by statically analyzing programs at load\ntime to ensure that they cannot corrupt or crash the system. sched_ext\nguarantees system integrity no matter what BPF scheduler is loaded, and\nprovides mechanisms to safely disable the current BPF scheduler and migrate\ntasks back to a trusted scheduler. For example, we also implement in-kernel\nsafety mechanisms to guarantee that a misbehaving scheduler cannot\nindefinitely starve tasks. BPF also enables sched_ext to significantly improve\niteration speed for running experiments. Loading and unloading a BPF scheduler\nis simply a matter of running and terminating a sched_ext binary."}),"\n",(0,t.jsx)(n.p,{children:'BPF also provides programs with a rich set of APIs, such as maps, kfuncs, and\nBPF helpers. In addition to providing useful building blocks to programs that\nrun entirely in kernel space (such as many of our example schedulers), these\nAPIs also allow programs to leverage user space in making scheduling decisions.\nSpecifically, the Atropos sample scheduler has a relatively simple weighted\nvtime or FIFO scheduling layer in BPF, paired with a load balancing component\nin userspace written in Rust. As described in more detail below, we also built\na more general user-space scheduling framework called "rhone" by leveraging\nvarious BPF features.'}),"\n",(0,t.jsxs)(n.p,{children:["On the other hand, BPF does have shortcomings, as can be plainly seen from the\ncomplexity in some of the example schedulers. scx_pair.bpf.c illustrates this\npoint well. To start, it requires a good amount of code to emulate\ncgroup-local-storage. In the kernel proper, this would simply be a matter of\nadding another pointer to the struct cgroup, but in BPF, it requires a complex\njuggling of data amongst multiple different maps, a good amount of boilerplate\ncode, and some unwieldy ",(0,t.jsx)(n.code,{children:"bpf_loop()"}),"'s and atomics. The code is also littered\nwith explicit and often unnecessary sanity checks to appease the verifier."]}),"\n",(0,t.jsxs)(n.p,{children:["That being said, BPF is being rapidly improved. For example, Yonghong Song\nrecently upstreamed a\n",(0,t.jsx)(n.a,{href:"https://lore.kernel.org/bpf/20221026042835.672317-1-yhs@fb.com/",children:"patch set"})," to\nadd a cgroup local storage map type, allowing scx_pair.bpf.c to be simplified.\nThere are plans to address other issues as well, such as providing\nstatically-verified locking, and avoiding the need for unnecessary sanity\nchecks. Addressing these shortcomings is a high priority for BPF, and as\nprogress continues to be made, we expect most deficiencies to be addressed in\nthe not-too-distant future."]}),"\n",(0,t.jsxs)(n.p,{children:["Yet another exploration advantage of sched_ext is helping widening the scope\nof experiments. For example, sched_ext makes it easy to defer CPU assignment\nuntil a task starts executing, allowing schedulers to share scheduling queues\nat any granularity (hyper-twin, CCX and so on). Additionally, higher level\nframeworks can be built on top to further widen the scope. For example, the\naforementioned ",(0,t.jsx)(n.a,{href:"https://github.com/Byte-Lab/rhone",children:"rhone"})," library allows\nimplementing scheduling policies in user-space by encapsulating the complexity\naround communicating scheduling decisions with the kernel. This allows taking\nadvantage of a richer programming environment in user-space, enabling\nexperimenting with, for instance, more complex mathematical models."]}),"\n",(0,t.jsx)(n.p,{children:"sched_ext also allows developers to leverage machine learning. At Meta, we\nexperimented with using machine learning to predict whether a running task\nwould soon yield its CPU. These predictions can be used to aid the scheduler in\ndeciding whether to keep a runnable task on its current CPU rather than\nmigrating it to an idle CPU, with the hope of avoiding unnecessary cache\nmisses. Using a tiny neural net model with only one hidden layer of size 16,\nand a decaying count of 64 syscalls as a feature, we were able to achieve a 15%\nthroughput improvement on an Nginx benchmark, with an 87% inference accuracy."}),"\n",(0,t.jsx)(n.h2,{id:"2-customization",children:"2. Customization"}),"\n",(0,t.jsx)(n.p,{children:"This section discusses how sched_ext can enable users to run workloads on\napplication-specific schedulers."}),"\n",(0,t.jsx)(n.h3,{id:"why-deploy-custom-schedulers-rather-than-improving-cfs",children:"Why deploy custom schedulers rather than improving CFS?"}),"\n",(0,t.jsx)(n.p,{children:"Implementing application-specific schedulers and improving CFS are not\nconflicting goals. Scheduling features explored with sched_ext which yield\nbeneficial results, and which are sufficiently generalizable, can and should\nbe integrated into CFS. However, CFS is fundamentally designed to be a general\npurpose scheduler, and thus is not conducive to being extended with some\nhighly targeted application or hardware specific changes."}),"\n",(0,t.jsx)(n.p,{children:"Targeted, bespoke scheduling has many potential use cases. For example, VM\nscheduling can make certain optimizations that are infeasible in CFS due to\nthe constrained problem space (scheduling a static number of long-running\nVCPUs versus an arbitrary number of threads). Additionally, certain\napplications might want to make targeted policy decisions based on hints\ndirectly from the application (for example, a service that knows the different\ndeadlines of incoming RPCs)."}),"\n",(0,t.jsx)(n.p,{children:'Google has also experimented with some promising, novel scheduling policies.\nOne example is "central" scheduling, wherein a single CPU makes all scheduling\ndecisions for the entire system. This allows most cores on the system to be\nfully dedicated to running workloads, and can have significant performance\nimprovements for certain use cases. For example, central scheduling with VCPUs\ncan avoid expensive vmexits and cache flushes, by instead delegating the\nresponsibility of preemption checks from the tick to a single CPU. See\nscx_central.bpf.c for a simple example of a central scheduling policy built in\nsched_ext.'}),"\n",(0,t.jsx)(n.p,{children:"Some workloads also have non-generalizable constraints which enable\noptimizations in a scheduling policy which would otherwise not be feasible.\nFor example,VM workloads at Google typically have a low overcommit ratio\ncompared to the number of physical CPUs. This allows the scheduler to support\nbounded tail latencies, as well as longer blocks of uninterrupted time."}),"\n",(0,t.jsxs)(n.p,{children:["Yet another interesting use case is the scx_flatcg scheduler, which provides a\nflattened hierarchical vtree for cgroups. This scheduler does not account for\nthundering herd problems among cgroups, and therefore may not be suitable for\ninclusion in CFS. However, in a simple benchmark using\n",(0,t.jsx)(n.a,{href:"https://github.com/wg/wrk",children:"wrk"})," on apache serving a CGI script calculating\nsha1sum of a small file, it outperformed CFS by ~3% with CPU controller\ndisabled and by ~10% with two apache instances competing with 2:1 weight ratio\nnested four level deep."]}),"\n",(0,t.jsxs)(n.p,{children:["Certain industries require specific scheduling behaviors that do not apply\nbroadly. For example, ARINC 653 defines scheduling behavior that is widely used\nby avionic software, and some out-of-tree implementations\n(",(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/7005306",children:"https://ieeexplore.ieee.org/document/7005306"}),") have been built. While the\nupstream community may decide to merge one such implementation in the future,\nit would also be entirely reasonable to not do so given the narrowness of\nuse-case, and non-generalizable, strict requirements. Such cases can be well\nserved by sched_ext in all stages of the software development lifecycle --\ndevelopment, testing, deployment and maintenance."]}),"\n",(0,t.jsx)(n.p,{children:"There are also classes of policy exploration, such as machine learning, or\nresponding in real-time to application hints, that are significantly harder\n(and not necessarily appropriate) to integrate within the kernel itself."}),"\n",(0,t.jsx)(n.h3,{id:"wont-this-increase-fragmentation",children:"Won't this increase fragmentation?"}),"\n",(0,t.jsx)(n.p,{children:"We acknowledge that to some degree, sched_ext does run the risk of increasing\nthe fragmentation of scheduler implementations. As a result of exploration,\nhowever, we believe that enabling the larger ecosystem to innovate will\nultimately accelerate the overall development and performance of Linux."}),"\n",(0,t.jsx)(n.p,{children:"BPF programs are required to be GPLv2, which is enforced by the verifier on\nprogram loads. With regards to API stability, just as with other semi-internal\ninterfaces such as BPF kfuncs, we won't be providing any API stability\nguarantees to BPF schedulers. While we intend to make an effort to provide\ncompatibility when possible, we will not provide any explicit, strong\nguarantees as the kernel typically does with e.g. UAPI headers. For users who\ndecide to keep their schedulers out-of-tree,the licensing and maintenance\noverheads will be fundamentally the same as for carrying out-of-tree patches."}),"\n",(0,t.jsx)(n.p,{children:"With regards to the schedulers included in this patch set, and any other\nschedulers we implement in the future, both Meta and Google will open-source\nall of the schedulers we implement which have any relevance to the broader\nupstream community. We expect that some of these, such as the simple example\nschedulers and scx_rusty scheduler, will be upstreamed as part of the kernel\ntree. Distros will be able to package and release these schedulers with the\nkernel, allowing users to utilize these schedulers out-of-the-box without\nrequiring any additional work or dependencies such as clang or building the\nscheduler programs themselves. Other schedulers and scheduling frameworks such\nas rhone may be open-sourced through separate per-project repos."}),"\n",(0,t.jsx)(n.h2,{id:"3-rapid-scheduler-deployments",children:"3. Rapid scheduler deployments"}),"\n",(0,t.jsxs)(n.p,{children:["Rolling out kernel upgrades is a slow and iterative process. At a large scale\nit can take months to roll a new kernel out to a fleet of servers. While this\nlatency is expected and inevitable for normal kernel upgrades, it can become\nhighly problematic when kernel changes are required to fix bugs.\n",(0,t.jsx)(n.a,{href:"https://www.kernel.org/doc/html/latest/livepatch/livepatch.html",children:"Livepatch"})," is\navailable to quickly roll out critical security fixes to large fleets, but the\nscope of changes that can be applied with livepatching is fairly limited, and\nwould likely not be usable for patching scheduling policies. With sched_ext,\nnew scheduling policies can be rapidly rolled out to production environments."]}),"\n",(0,t.jsxs)(n.p,{children:["As an example, one of the variants of the ",(0,t.jsx)(n.a,{href:"https://www.intel.com/content/www/us/en/architecture-and-technology/l1tf.html",children:"L1 Terminal Fault\n(L1TF)"}),'\nvulnerability allows a VCPU running a VM to read arbitrary host kernel memory\nfor pages in L1 data cache. The solution was to implement core scheduling,\nwhich ensures that tasks running as hypertwins have the same "cookie".']}),"\n",(0,t.jsx)(n.p,{children:"While core scheduling works well, it took a long time to finalize and land\nupstream. This long rollout period was painful, and required organizations to\nmake difficult choices amongst a bad set of options. Some companies such as\nGoogle chose to implement and use their own custom L1TF-safe scheduler, others\nchose to run without hyper-threading enabled, and yet others left\nhyper-threading enabled and crossed their fingers."}),"\n",(0,t.jsx)(n.p,{children:"Once core scheduling was upstream, organizations had to upgrade the kernels on\ntheir entire fleets. As downtime is not an option for many, these upgrades had\nto be gradually rolled out, which can take a very long time for large fleets."}),"\n",(0,t.jsx)(n.p,{children:"An example of an sched_ext scheduler that illustrates core scheduling semantics\nis scx_pair.bpf.c, which co-schedules pairs of tasks from the same cgroup, and\nis resilient to L1TF vulnerabilities. While this example scheduler is certainly\nnot suitable for production in its current form, a similar scheduler that is\nmore performant and featureful could be written and deployed if necessary."}),"\n",(0,t.jsx)(n.p,{children:"Rapid scheduling deployments can similarly be useful to quickly roll-out new\nscheduling features without requiring kernel upgrades. At Google, for example,\nit was observed that some low-priority workloads were causing degraded\nperformance for higher-priority workloads due to consuming a disproportionate\nshare of memory bandwidth. While a temporary mitigation was to use sched\naffinity to limit the footprint of this low-priority workload to a small subset\nof CPUs, a preferable solution would be to implement a more featureful\ntask-priority mechanism which automatically throttles lower-priority tasks\nwhich are causing memory contention for the rest of the system. Implementing\nthis in CFS and rolling it out to the fleet could take a very long time."}),"\n",(0,t.jsx)(n.p,{children:"sched_ext would directly address these gaps. If another hardware bug or\nresource contention issue comes in that requires scheduler support to mitigate,\nsched_ext can be used to experiment with and test different policies. Once a\nscheduler is available, it can quickly be rolled out to as many hosts as\nnecessary, and function as a stop-gap solution until a longer-term mitigation\nis upstreamed."}),"\n",(0,t.jsx)(n.h1,{id:"how",children:"How"}),"\n",(0,t.jsx)(n.p,{children:"sched_ext is a new sched_class which allows scheduling policies to be\nimplemented in BPF programs."}),"\n",(0,t.jsx)(n.p,{children:"sched_ext leverages BPF's struct_ops feature to define a structure which\nexports function callbacks and flags to BPF programs that wish to implement\nscheduling policies. The struct_ops structure exported by sched_ext is struct\nsched_ext_ops, and is conceptually similar to struct sched_class. The role of\nsched_ext is to map the complex sched_class callbacks to the more simple and\nergonomic struct sched_ext_ops callbacks."}),"\n",(0,t.jsx)(n.p,{children:"Unlike some other BPF program types which have ABI requirements due to\nexporting UAPIs, struct_ops has no ABI requirements whatsoever. This provides\nus with the flexibility to change the APIs provided to schedulers as necessary.\nBPF struct_ops is also already being used successfully in other subsystems,\nsuch as in support of TCP congestion control."}),"\n",(0,t.jsx)(n.p,{children:"The only struct_ops field that is required to be specified by a scheduler is\nthe 'name' field. Otherwise, sched_ext will provide sane default behavior, such\nas automatically choosing an idle CPU on the task wakeup path if .select_cpu()\nis missing."}),"\n",(0,t.jsx)(n.h3,{id:"dispatch-queues",children:"Dispatch queues"}),"\n",(0,t.jsxs)(n.p,{children:["To match the impedance between the scheduler core and the BPF scheduler,\nsched_ext uses DSQs (dispatch queues) which can operate as both a FIFO and a\npriority queue. By default, there is one global FIFO (",(0,t.jsx)(n.code,{children:"SCX_DSQ_GLOBAL"}),"),\nand one local dsq per CPU (",(0,t.jsx)(n.code,{children:"SCX_DSQ_LOCAL"}),"). The BPF scheduler can manage\nan arbitrary number of dsq's using ",(0,t.jsx)(n.code,{children:"scx_bpf_create_dsq()"})," and\n",(0,t.jsx)(n.code,{children:"scx_bpf_destroy_dsq()"}),"."]}),"\n",(0,t.jsx)(n.p,{children:'A CPU always executes a task from its local DSQ. A task is "dispatched" to a\nDSQ. A non-local DSQ is "consumed" to transfer a task to the consuming CPU\'s\nlocal DSQ.'}),"\n",(0,t.jsxs)(n.p,{children:["When a CPU is looking for the next task to run, if the local DSQ is not\nempty, the first task is picked. Otherwise, the CPU tries to consume the\nglobal DSQ. If that doesn't yield a runnable task either, ",(0,t.jsx)(n.code,{children:"ops.dispatch()"}),"\nis invoked."]}),"\n",(0,t.jsx)(n.h3,{id:"scheduling-cycle",children:"Scheduling cycle"}),"\n",(0,t.jsx)(n.p,{children:"The following briefly shows how a waking task is scheduled and executed."}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["When a task is waking up, ",(0,t.jsx)(n.code,{children:"ops.select_cpu()"})," is the first operation\ninvoked. This serves two purposes. First, CPU selection optimization\nhint. Second, waking up the selected CPU if idle."]}),"\n",(0,t.jsxs)(n.p,{children:["The CPU selected by ",(0,t.jsx)(n.code,{children:"ops.select_cpu()"})," is an optimization hint and not\nbinding. The actual decision is made at the last step of scheduling.\nHowever, there is a small performance gain if the CPU\n",(0,t.jsx)(n.code,{children:"ops.select_cpu()"})," returns matches the CPU the task eventually runs on."]}),"\n",(0,t.jsxs)(n.p,{children:["A side-effect of selecting a CPU is waking it up from idle. While a BPF\nscheduler can wake up any cpu using the ",(0,t.jsx)(n.code,{children:"scx_bpf_kick_cpu()"})," helper,\nusing ",(0,t.jsx)(n.code,{children:"ops.select_cpu()"})," judiciously can be simpler and more efficient."]}),"\n",(0,t.jsxs)(n.p,{children:["A task can be immediately dispatched to a DSQ from ",(0,t.jsx)(n.code,{children:"ops.select_cpu()"})," by\ncalling ",(0,t.jsx)(n.code,{children:"scx_bpf_dispatch()"}),". If the task is dispatched to\n",(0,t.jsx)(n.code,{children:"SCX_DSQ_LOCAL"})," from ",(0,t.jsx)(n.code,{children:"ops.select_cpu()"}),", it will be dispatched to the\nlocal DSQ of whichever CPU is returned from ",(0,t.jsx)(n.code,{children:"ops.select_cpu()"}),".\nAdditionally, dispatching directly from ",(0,t.jsx)(n.code,{children:"ops.select_cpu()"})," will cause the\n",(0,t.jsx)(n.code,{children:"ops.enqueue()"})," callback to be skipped."]}),"\n",(0,t.jsx)(n.p,{children:"Note that the scheduler core will ignore an invalid CPU selection, for\nexample, if it's outside the allowed cpumask of the task."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Once the target CPU is selected, ",(0,t.jsx)(n.code,{children:"ops.enqueue()"})," is invoked (unless the\ntask was dispatched directly from ",(0,t.jsx)(n.code,{children:"ops.select_cpu()"}),"). ",(0,t.jsx)(n.code,{children:"ops.enqueue()"}),"\ncan make one of the following decisions:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Immediately dispatch the task to either the global or local DSQ by\ncalling ",(0,t.jsx)(n.code,{children:"scx_bpf_dispatch()"})," with ",(0,t.jsx)(n.code,{children:"SCX_DSQ_GLOBAL"})," or\n",(0,t.jsx)(n.code,{children:"SCX_DSQ_LOCAL"}),", respectively."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Immediately dispatch the task to a custom DSQ by calling\n",(0,t.jsx)(n.code,{children:"scx_bpf_dispatch()"})," with a DSQ ID which is smaller than 2^63."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Queue the task on the BPF side."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["When a CPU is ready to schedule, it first looks at its local DSQ. If\nempty, it then looks at the global DSQ. If there still isn't a task to\nrun, ",(0,t.jsx)(n.code,{children:"ops.dispatch()"})," is invoked which can use the following two\nfunctions to populate the local DSQ."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"scx_bpf_dispatch()"})," dispatches a task to a DSQ. Any target DSQ can\nbe used - ",(0,t.jsx)(n.code,{children:"SCX_DSQ_LOCAL"}),", ",(0,t.jsx)(n.code,{children:"SCX_DSQ_LOCAL_ON | cpu"}),",\n",(0,t.jsx)(n.code,{children:"SCX_DSQ_GLOBAL"})," or a custom DSQ. While ",(0,t.jsx)(n.code,{children:"scx_bpf_dispatch()"}),"\ncurrently can't be called with BPF locks held, this is being worked on\nand will be supported. ",(0,t.jsx)(n.code,{children:"scx_bpf_dispatch()"})," schedules dispatching\nrather than performing them immediately. There can be up to\n",(0,t.jsx)(n.code,{children:"ops.dispatch_max_batch"})," pending tasks."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"scx_bpf_consume()"})," tranfers a task from the specified non-local DSQ\nto the dispatching DSQ. This function cannot be called with any BPF\nlocks held. ",(0,t.jsx)(n.code,{children:"scx_bpf_consume()"})," flushes the pending dispatched tasks\nbefore trying to consume the specified DSQ."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["After ",(0,t.jsx)(n.code,{children:"ops.dispatch()"})," returns, if there are tasks in the local DSQ,\nthe CPU runs the first one. If empty, the following steps are taken:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Try to consume the global DSQ. If successful, run the task."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["If ",(0,t.jsx)(n.code,{children:"ops.dispatch()"})," has dispatched any tasks, retry #3."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["If the previous task is an SCX task and still runnable, keep executing\nit (see ",(0,t.jsx)(n.code,{children:"SCX_OPS_ENQ_LAST"}),")."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Go idle."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Note that the BPF scheduler can always choose to dispatch tasks immediately in\n",(0,t.jsx)(n.code,{children:"ops.select_cpu()"})," or ",(0,t.jsx)(n.code,{children:"ops.enqueue()"}),". If only the built-in DSQs are used,\nthere is no need to implement ",(0,t.jsx)(n.code,{children:"ops.dispatch()"})," as a task is never queued on\nthe BPF scheduler and both the local and global DSQs are consumed\nautomatically."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"scx_bpf_dispatch()"})," queues the task on the FIFO of the target DSQ. Use\n",(0,t.jsx)(n.code,{children:"scx_bpf_dispatch_vtime()"})," for the priority queue. Internal DSQs such as\n",(0,t.jsx)(n.code,{children:"SCX_DSQ_LOCAL"})," and ",(0,t.jsx)(n.code,{children:"SCX_DSQ_GLOBAL"})," do not support priority-queue\ndispatching, and must be dispatched to with ",(0,t.jsx)(n.code,{children:"scx_bpf_dispatch()"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"verifying-callback-behavior",children:"Verifying callback behavior"}),"\n",(0,t.jsxs)(n.p,{children:["sched_ext always verifies that any value returned from a callback is valid, and\nwill issue an error and unload the scheduler if it is not. For example, if\n",(0,t.jsx)(n.code,{children:".select_cpu()"})," returns an invalid CPU, or if an attempt is made to invoke the\n",(0,t.jsx)(n.code,{children:"scx_bpf_dispatch()"})," with invalid enqueue flags. Furthermore, if a task remains\nrunnable for too long without being scheduled, sched_ext will detect it and\nerror-out the scheduler."]}),"\n",(0,t.jsx)(n.h1,{id:"closing-thoughts",children:"Closing Thoughts"}),"\n",(0,t.jsx)(n.p,{children:"Both Meta and Google have experimented quite a lot with schedulers in the last\nseveral years. Google has benchmarked various workloads using user space\nscheduling, and have achieved performance wins by trading off generality for\napplication specific needs. At Meta, we are actively experimenting with\nmultiple production workloads and seeing significant performance gains, and are\nin the process of deploying sched_ext schedulers on production workloads at\nscale. We expect to leverage it extensively to run various experiments and\ndevelop customized schedulers for a number of critical workloads."}),"\n",(0,t.jsx)(n.h1,{id:"written-by",children:"Written By"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["David Vernet ",(0,t.jsx)(n.a,{href:"mailto:dvernet@meta.com",children:"dvernet@meta.com"})]}),"\n",(0,t.jsxs)(n.li,{children:["Josh Don ",(0,t.jsx)(n.a,{href:"mailto:joshdon@google.com",children:"joshdon@google.com"})]}),"\n",(0,t.jsxs)(n.li,{children:["Tejun Heo ",(0,t.jsx)(n.a,{href:"mailto:tj@kernel.org",children:"tj@kernel.org"})]}),"\n",(0,t.jsxs)(n.li,{children:["Barret Rhoden ",(0,t.jsx)(n.a,{href:"mailto:brho@google.com",children:"brho@google.com"})]}),"\n"]}),"\n",(0,t.jsx)(n.h1,{id:"supported-by",children:"Supported By"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Paul Turner ",(0,t.jsx)(n.a,{href:"mailto:pjt@google.com",children:"pjt@google.com"})]}),"\n",(0,t.jsxs)(n.li,{children:["Neel Natu ",(0,t.jsx)(n.a,{href:"mailto:neelnatu@google.com",children:"neelnatu@google.com"})]}),"\n",(0,t.jsxs)(n.li,{children:["Patrick Bellasi ",(0,t.jsx)(n.a,{href:"mailto:derkling@google.com",children:"derkling@google.com"})]}),"\n",(0,t.jsxs)(n.li,{children:["Hao Luo ",(0,t.jsx)(n.a,{href:"mailto:haoluo@google.com",children:"haoluo@google.com"})]}),"\n",(0,t.jsxs)(n.li,{children:["Dimitrios Skarlatos ",(0,t.jsx)(n.a,{href:"mailto:dskarlat@cs.cmu.edu",children:"dskarlat@cs.cmu.edu"})]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>r});var i=s(6540);const t={},a=i.createContext(t);function o(e){const n=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);